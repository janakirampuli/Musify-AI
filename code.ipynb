{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 18:15:42.709159: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 18:15:42.709191: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#library for understanding music\n",
    "from music21 import *\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import LSTM,Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse and Read Midi Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]/home/puli/SoC/musify/musify-env/lib/python3.9/site-packages/music21/midi/translate.py:883: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=4, channel=None, data=b'Copyright \\xa9 2001 by Bernd Kr\\xfcger'>; getting generic Instrument\n",
      "  warnings.warn(\n",
      "100%|██████████| 14/14 [01:11<00:00,  5.13s/it]\n",
      "/tmp/ipykernel_19640/2725945799.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  notes_array = np.array([read_midi_file(path+i) for i in tqdm(files)])\n"
     ]
    }
   ],
   "source": [
    "def read_midi_file(file):\n",
    "    notes = []\n",
    "    notes_to_parse = None\n",
    "\n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "\n",
    "    #grouping based on different instruments\n",
    "    ins = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    #Looping over all the instruments\n",
    "    for part in ins.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "    return notes\n",
    "\n",
    "#specify the path\n",
    "path='All Midi Files/albeniz/'\n",
    "\n",
    "#read all the filenames\n",
    "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_midi_file(path+i) for i in tqdm(files)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique notes:  248\n",
      "\n",
      "Frequency notes\n",
      "30 : 124\n",
      "50 : 92\n",
      "70 : 71\n",
      "90 : 53\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 302,
       "width": 313
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#converting 2D array into 1D array\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "\n",
    "#No. of unique notes\n",
    "unique_notes = list(set(notes_))\n",
    "print(\"No. of unique notes: \", len(unique_notes))\n",
    "\n",
    "#notes with their frequency\n",
    "freq=dict(map(lambda x: (x,notes_.count(x)),unique_notes))\n",
    "\n",
    "#get the threshold frequency\n",
    "print(\"\\nFrequency notes\")\n",
    "for i in range(30,100,20):\n",
    "  print(i,\":\",len(list(filter(lambda x:x[1]>=i,freq.items()))))\n",
    "\n",
    "#filter notes greater than threshold i.e. 50\n",
    "freq_notes=dict(filter(lambda x:x[1]>=50,freq.items()))\n",
    "\n",
    "# consider only the frequencies\n",
    "no=[count for _, count in freq_notes.items()]\n",
    "\n",
    "#set the figure size\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#plot\n",
    "plt.hist(no)\n",
    "\n",
    "#create new notes using the frequent notes\n",
    "new_music=[[note_ for note_ in notes if note_ in freq_notes] for notes in notes_array]\n",
    "\n",
    "#dictionary having key as note index and value as note\n",
    "ind2note=dict(enumerate(freq_notes))\n",
    "\n",
    "#dictionary having key as note and value as note index\n",
    "note2ind=dict(map(reversed,ind2note.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the input and output sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_timesteps = 35\n",
    "\n",
    "#store values of input and output\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        #input will be the current index + timestep\n",
    "        #output will be the next index after timestep\n",
    "\n",
    "        inp = note_[i:i + no_of_timesteps]\n",
    "        out = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(list(map(lambda x:note2ind[x],inp)))\n",
    "        y.append(note2ind[out])\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape input and output for the model\n",
    "x_seq = np.reshape(x,(len(x),no_of_timesteps,1))\n",
    "y_seq = np.reshape(y,(-1,1))\n",
    "\n",
    "#split the input and value into training and testing sets\n",
    "#80% for training and 20% for testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_seq, y_seq, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 18:17:45.555147: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-07-17 18:17:45.555258: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Puli): /proc/driver/nvidia/version does not exist\n",
      "2022-07-17 18:17:45.823100: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 35, 256)           264192    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 35, 256)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 92)                23644     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 878,940\n",
      "Trainable params: 878,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create the model\n",
    "model = Sequential()\n",
    "#create two stacked LSTM layer with the latent dimension of 256\n",
    "model.add(LSTM(256,return_sequences=True,input_shape=(x_seq.shape[1],x_seq.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "\n",
    "#fully connected layer for the output with softmax activation\n",
    "model.add(Dense(len(note2ind),activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "92/92 [==============================] - 50s 473ms/step - loss: 4.2195 - accuracy: 0.0740 - val_loss: 4.1822 - val_accuracy: 0.0637\n",
      "Epoch 2/50\n",
      "92/92 [==============================] - 42s 455ms/step - loss: 4.1235 - accuracy: 0.0775 - val_loss: 4.1140 - val_accuracy: 0.0658\n",
      "Epoch 3/50\n",
      "92/92 [==============================] - 49s 529ms/step - loss: 4.0524 - accuracy: 0.0811 - val_loss: 4.0539 - val_accuracy: 0.0750\n",
      "Epoch 4/50\n",
      "92/92 [==============================] - 48s 519ms/step - loss: 3.9511 - accuracy: 0.0945 - val_loss: 3.9147 - val_accuracy: 0.0961\n",
      "Epoch 5/50\n",
      "92/92 [==============================] - 47s 514ms/step - loss: 3.8209 - accuracy: 0.1115 - val_loss: 3.8061 - val_accuracy: 0.1155\n",
      "Epoch 6/50\n",
      "92/92 [==============================] - 49s 536ms/step - loss: 3.6922 - accuracy: 0.1342 - val_loss: 3.6888 - val_accuracy: 0.1288\n",
      "Epoch 7/50\n",
      "92/92 [==============================] - 57s 616ms/step - loss: 3.5428 - accuracy: 0.1553 - val_loss: 3.5575 - val_accuracy: 0.1626\n",
      "Epoch 8/50\n",
      "92/92 [==============================] - 52s 560ms/step - loss: 3.3929 - accuracy: 0.1796 - val_loss: 3.4434 - val_accuracy: 0.1810\n",
      "Epoch 9/50\n",
      "92/92 [==============================] - 51s 558ms/step - loss: 3.2194 - accuracy: 0.2061 - val_loss: 3.3294 - val_accuracy: 0.1970\n",
      "Epoch 10/50\n",
      "92/92 [==============================] - 51s 553ms/step - loss: 3.0749 - accuracy: 0.2274 - val_loss: 3.2361 - val_accuracy: 0.2100\n",
      "Epoch 11/50\n",
      "92/92 [==============================] - 51s 555ms/step - loss: 2.9141 - accuracy: 0.2564 - val_loss: 3.1252 - val_accuracy: 0.2427\n",
      "Epoch 12/50\n",
      "92/92 [==============================] - 51s 559ms/step - loss: 2.7539 - accuracy: 0.2879 - val_loss: 3.0509 - val_accuracy: 0.2509\n",
      "Epoch 13/50\n",
      "92/92 [==============================] - 51s 557ms/step - loss: 2.5904 - accuracy: 0.3178 - val_loss: 3.0099 - val_accuracy: 0.2584\n",
      "Epoch 14/50\n",
      "92/92 [==============================] - 53s 573ms/step - loss: 2.4320 - accuracy: 0.3545 - val_loss: 2.9271 - val_accuracy: 0.2856\n",
      "Epoch 15/50\n",
      "92/92 [==============================] - 66s 722ms/step - loss: 2.2682 - accuracy: 0.3890 - val_loss: 2.8788 - val_accuracy: 0.3010\n",
      "Epoch 16/50\n",
      "92/92 [==============================] - 57s 626ms/step - loss: 2.1443 - accuracy: 0.4164 - val_loss: 2.8196 - val_accuracy: 0.3149\n",
      "Epoch 17/50\n",
      "92/92 [==============================] - 59s 647ms/step - loss: 1.9951 - accuracy: 0.4543 - val_loss: 2.7884 - val_accuracy: 0.3333\n",
      "Epoch 18/50\n",
      "92/92 [==============================] - 46s 503ms/step - loss: 1.8731 - accuracy: 0.4793 - val_loss: 2.7398 - val_accuracy: 0.3466\n",
      "Epoch 19/50\n",
      "92/92 [==============================] - 54s 585ms/step - loss: 1.7461 - accuracy: 0.5092 - val_loss: 2.7325 - val_accuracy: 0.3575\n",
      "Epoch 20/50\n",
      "92/92 [==============================] - 47s 512ms/step - loss: 1.6293 - accuracy: 0.5325 - val_loss: 2.7314 - val_accuracy: 0.3555\n",
      "Epoch 21/50\n",
      "92/92 [==============================] - 48s 516ms/step - loss: 1.5265 - accuracy: 0.5573 - val_loss: 2.7213 - val_accuracy: 0.3759\n",
      "Epoch 22/50\n",
      "92/92 [==============================] - 48s 516ms/step - loss: 1.4221 - accuracy: 0.5881 - val_loss: 2.6890 - val_accuracy: 0.3909\n",
      "Epoch 23/50\n",
      "92/92 [==============================] - 47s 514ms/step - loss: 1.3344 - accuracy: 0.6049 - val_loss: 2.6792 - val_accuracy: 0.4066\n",
      "Epoch 24/50\n",
      "92/92 [==============================] - 48s 521ms/step - loss: 1.2375 - accuracy: 0.6358 - val_loss: 2.7122 - val_accuracy: 0.4022\n",
      "Epoch 25/50\n",
      "92/92 [==============================] - 48s 517ms/step - loss: 1.1524 - accuracy: 0.6582 - val_loss: 2.7020 - val_accuracy: 0.4155\n",
      "Epoch 26/50\n",
      "92/92 [==============================] - 51s 559ms/step - loss: 1.0750 - accuracy: 0.6787 - val_loss: 2.7511 - val_accuracy: 0.4175\n",
      "Epoch 27/50\n",
      "92/92 [==============================] - 53s 579ms/step - loss: 1.0210 - accuracy: 0.6886 - val_loss: 2.7293 - val_accuracy: 0.4271\n",
      "Epoch 28/50\n",
      "92/92 [==============================] - 51s 555ms/step - loss: 0.9478 - accuracy: 0.7137 - val_loss: 2.7504 - val_accuracy: 0.4346\n",
      "Epoch 29/50\n",
      "92/92 [==============================] - 64s 696ms/step - loss: 0.8753 - accuracy: 0.7342 - val_loss: 2.7771 - val_accuracy: 0.4335\n",
      "Epoch 30/50\n",
      "92/92 [==============================] - 70s 754ms/step - loss: 0.8309 - accuracy: 0.7485 - val_loss: 2.8067 - val_accuracy: 0.4461\n",
      "Epoch 31/50\n",
      "92/92 [==============================] - 57s 620ms/step - loss: 0.7815 - accuracy: 0.7602 - val_loss: 2.8519 - val_accuracy: 0.4461\n",
      "Epoch 32/50\n",
      "92/92 [==============================] - 62s 673ms/step - loss: 0.7346 - accuracy: 0.7765 - val_loss: 2.8984 - val_accuracy: 0.4557\n",
      "Epoch 33/50\n",
      "92/92 [==============================] - 56s 606ms/step - loss: 0.6874 - accuracy: 0.7853 - val_loss: 2.9013 - val_accuracy: 0.4530\n",
      "Epoch 34/50\n",
      "92/92 [==============================] - 51s 557ms/step - loss: 0.6429 - accuracy: 0.8003 - val_loss: 2.9380 - val_accuracy: 0.4475\n",
      "Epoch 35/50\n",
      "92/92 [==============================] - 63s 688ms/step - loss: 0.6027 - accuracy: 0.8143 - val_loss: 2.9430 - val_accuracy: 0.4594\n",
      "Epoch 36/50\n",
      "92/92 [==============================] - 64s 694ms/step - loss: 0.5739 - accuracy: 0.8232 - val_loss: 3.0289 - val_accuracy: 0.4622\n",
      "Epoch 37/50\n",
      "92/92 [==============================] - 60s 648ms/step - loss: 0.5427 - accuracy: 0.8314 - val_loss: 3.0478 - val_accuracy: 0.4635\n",
      "Epoch 38/50\n",
      "92/92 [==============================] - 67s 725ms/step - loss: 0.5162 - accuracy: 0.8439 - val_loss: 3.0303 - val_accuracy: 0.4710\n",
      "Epoch 39/50\n",
      "92/92 [==============================] - 67s 727ms/step - loss: 0.4861 - accuracy: 0.8500 - val_loss: 3.1113 - val_accuracy: 0.4673\n",
      "Epoch 40/50\n",
      "92/92 [==============================] - 87s 945ms/step - loss: 0.4732 - accuracy: 0.8536 - val_loss: 3.1217 - val_accuracy: 0.4686\n",
      "Epoch 41/50\n",
      "92/92 [==============================] - 81s 880ms/step - loss: 0.4488 - accuracy: 0.8625 - val_loss: 3.1417 - val_accuracy: 0.4673\n",
      "Epoch 42/50\n",
      "92/92 [==============================] - 59s 646ms/step - loss: 0.4391 - accuracy: 0.8617 - val_loss: 3.1846 - val_accuracy: 0.4755\n",
      "Epoch 43/50\n",
      "92/92 [==============================] - 52s 570ms/step - loss: 0.4155 - accuracy: 0.8714 - val_loss: 3.1941 - val_accuracy: 0.4748\n",
      "Epoch 44/50\n",
      "92/92 [==============================] - 59s 645ms/step - loss: 0.3941 - accuracy: 0.8781 - val_loss: 3.2597 - val_accuracy: 0.4751\n",
      "Epoch 45/50\n",
      "92/92 [==============================] - 70s 764ms/step - loss: 0.3878 - accuracy: 0.8797 - val_loss: 3.2579 - val_accuracy: 0.4761\n",
      "Epoch 46/50\n",
      "92/92 [==============================] - 59s 646ms/step - loss: 0.3657 - accuracy: 0.8852 - val_loss: 3.3048 - val_accuracy: 0.4819\n",
      "Epoch 47/50\n",
      "92/92 [==============================] - 67s 729ms/step - loss: 0.3518 - accuracy: 0.8884 - val_loss: 3.3360 - val_accuracy: 0.4734\n",
      "Epoch 48/50\n",
      "92/92 [==============================] - 63s 688ms/step - loss: 0.3482 - accuracy: 0.8940 - val_loss: 3.3582 - val_accuracy: 0.4724\n",
      "Epoch 49/50\n",
      "92/92 [==============================] - 53s 581ms/step - loss: 0.3282 - accuracy: 0.9000 - val_loss: 3.3721 - val_accuracy: 0.4905\n",
      "Epoch 50/50\n",
      "92/92 [==============================] - 48s 525ms/step - loss: 0.3225 - accuracy: 0.8990 - val_loss: 3.4075 - val_accuracy: 0.4826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2224ca4a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compile the model using Adam optimizer\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "#train the model on training sets and validate on testing sets\n",
    "model.fit(\n",
    "    x_train,y_train,\n",
    "    batch_size=128,epochs=50,\n",
    "    validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MODEL/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MODEL/assets\n"
     ]
    }
   ],
   "source": [
    "#saving the Model\n",
    "model.save(\"MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interference Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "model=load_model(\"MODEL\")\n",
    "#generate random index\n",
    "index = np.random.randint(0,len(x_test)-1)\n",
    "#get the data of generated index from x_test\n",
    "music_pattern = x_test[index]\n",
    "\n",
    "out_pred=[] #it will store predicted notes\n",
    "\n",
    "#iterate till 200 note is generated\n",
    "for i in range(200):\n",
    "\n",
    "  #reshape the music pattern \n",
    "  music_pattern = music_pattern.reshape(1,len(music_pattern),1)\n",
    "  \n",
    "  #get the maximum probability value from the predicted output\n",
    "  pred_index = np.argmax(model.predict(music_pattern))\n",
    "  #get the note using predicted index and\n",
    "  #append to the output prediction list\n",
    "  out_pred.append(ind2note[pred_index])\n",
    "  music_pattern = np.append(music_pattern,pred_index)\n",
    "  \n",
    "  #update the music pattern with one timestep ahead\n",
    "  music_pattern = music_pattern[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI_composed_music.mid'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_notes = []\n",
    "for offset,pattern in enumerate(out_pred):\n",
    "  #if pattern is a chord instance\n",
    "  if ('.' in pattern) or pattern.isdigit():\n",
    "    #split notes from the chord\n",
    "    notes_in_chord = pattern.split('.')\n",
    "    notes = []\n",
    "    for current_note in notes_in_chord:\n",
    "        i_curr_note=int(current_note)\n",
    "        #cast the current note to Note object and\n",
    "        #append the current note \n",
    "        new_note = note.Note(i_curr_note)\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        notes.append(new_note)\n",
    "    \n",
    "    #cast the current note to Chord object\n",
    "    #offset will be 1 step ahead from the previous note\n",
    "    #as it will prevent notes to stack up \n",
    "    new_chord = chord.Chord(notes)\n",
    "    new_chord.offset = offset\n",
    "    output_notes.append(new_chord)\n",
    "  \n",
    "  else:\n",
    "    #cast the pattern to Note object apply the offset and \n",
    "    #append the note\n",
    "    new_note = note.Note(pattern)\n",
    "    new_note.offset = offset\n",
    "    new_note.storedInstrument = instrument.Piano()\n",
    "    output_notes.append(new_note)\n",
    "\n",
    "#save the midi file \n",
    "midi_stream = stream.Stream(output_notes)\n",
    "midi_stream.write('midi', fp='AI_composed_music.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('musify-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f98c4d485340e4e1dde2350ddc778649d40461c400cf1c661d7e002d1a150e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
